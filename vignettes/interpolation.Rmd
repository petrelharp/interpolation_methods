---
title: "Interpolation"
author: "Peter Ralph"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r doc_setup, include=FALSE}
library(interposim)
library(MASS)  # for mvrnorm
fig.dim <- 5
knitr::opts_chunk$set(fig.width=3*fig.dim,fig.height=fig.dim,fig.align='center')
set.seed(42)
```

# High-dimensional, noisy interpolation


## The model

Suppose that we have noisy observations of a function from $\mathbb{R}^k \to \mathbb{R}^n$,
with $n \gg k$:
$$\begin{aligned}
  (y_{i1}, \ldots, y_{in}) &= f(x_{i1}, \ldots, x_{ik}) + \epsilon_i \\
  \epsilon_i \sim N(0, \Sigma) .
\end{aligned}$$

Then, given a new output value $\tilde y$, estimate the corresponding $\tilde x$.


## Test case

Let's take $k=1$ and let
$$\begin{aligned}
    f(x) = (1/(1+x), 1/(1+x)^2, \ldots, 1/(1+x)^n)
\end{aligned}$$
for $0 \le x \le 1$.
We will also take uncorrelated noise, $\Sigma = \sigma^2 I$.

```{r sim_basic_test}
n <- 5
nx <- 100
Sigma <- .001 * diag(n)
xx <- runif(nx)
f <- function (x) { sapply(1:n, function (u) { 1/(1+x)^u }) }
fnoise <- function (x) { f(x) + mvrnorm(NROW(x), mu=rep(0,n), Sigma=Sigma) }
yy <- fnoise(xx)
matplot(xx[order(xx)], yy[order(xx),], type='l', 
        xlab='x', ylab='y', lty=1)
matlines(xx[order(xx)], f(xx[order(xx)]), lty=3)
```


## Local interpolation

A first guess is to take a weighted average over the reference values of $x$,
weighted according to their proximity to the observed $y$.
For instance,
$$\begin{aligned}
    \tilde x &= \sum_i x_i 
    \frac{\exp\left(-\|\tilde y - y_i\|^2 / 2 \omega^2\right)}
         {\sum_j \exp\left(-\|\tilde y - y_j\|^2 / 2 \omega^2\right)}
\end{aligned}$$
where $\omega$ is chosen appropriately.

```{r local_interp}
inverse_interpolation
```

Let's check this works, 
setting $\omega$ to include roughly 10\% of the nearest points.
```{r test_basic_case}
xref <- seq(0.1,0.9, length.out=8)
yref <- fnoise(xref)
xpred <- inverse_interpolation(xx, yy, yref, omega=0.05)
layout(t(1:2))
matplot(xx[order(xx)], yy[order(xx),], type='l', 
        xlab='x', ylab='y', lty=1)
matlines(xx[order(xx)], f(xx[order(xx)]), lty=3)
abline(v=xref, lty=4)
matpoints(xref, yref, pch=20)

matpoints(xpred, yref, pch=23)
segments(x0=xref, x1=xpred, y0=yref)
legend("topright", pch=c(20,1), legend=c("truth", "predicted"))

plot(xref, xpred, xlab='true x', ylab='predicted x')
abline(0,1)
```

### Error quantification

How well is this doing?  Let's see.
```{r error_basic_case, fig.width=fig.dim}
ntest <- 1000
xref <- runif(ntest)
yref <- fnoise(xref)
xpred <- inverse_interpolation(xx, yy, yref, omega=0.05)
plot(xref, xpred, xlab='true x', ylab='predicted x', asp=1)
abline(0,1)
```
The error varies with the true $x$, 
which makes sense since the curves flatten as $x$ increases.

How does this depend on the number of training samples:
```{r error_with_n}
ntest <- 1000
nvals <- 5*(1:20)
resid_list <- lapply(nvals, function (nx) {
                xx <- runif(nx)
                f <- function (x) { sapply(1:n, function (u) { 1/(1+x)^u }) }
                fnoise <- function (x) { f(x) + mvrnorm(NROW(x), mu=rep(0,n), Sigma=Sigma) }
                yy <- fnoise(xx)
                xref <- runif(ntest)
                yref <- fnoise(xref)
                xpred <- inverse_interpolation(xx, yy, yref, omega=0.05)
                return(xpred-xref)
            } )
names(resid_list) <- nvals
boxplot(resid_list, xlab='number of training samples', ylab='error')
```


### Automatic bandwidth selection

The bandwidth $\omega$ that minimizes error will depend on the data.
the plot above showed, the best bandwidth might vary as a function of the data $x$, but for now just consider choosing a single scalar "best" $\omega$.
We can do this by $k$-fold cross validation: 

* divide the data $x$ and $y$ into $k$ blocks, randomly
- for each of value in a sequence of bandwidths $\omega \in (\omega_l,  \omega_h)$:
* compute 
left(-\|\tilde y - y_i\|)

```{r optimal_local_interp}
automatic_inverse_interpolation_general
```

For the case above, we set $\omega=0.05$ (including roughly 10\% of the nearest points) 


```{r test-best-with-cv}
AE <- function(y, yhat) abs(y - yhat)
set.seed(111)
xbest <- automatic_inverse_interpolation_general(xx, yy, loss=AE, min_omega=.75, max_omega=1)
stopifnot(xbest$boundary_value)

set.seed(111)
xbest <- automatic_inverse_interpolation_general(cbind(xx), yy, loss=AE, min_omega=.0001, max_omega=1)
stopifnot(!xbest$boundary_value)
```

## With correlation

Now let's see what happens if there is a lot of unmodeled correlation in the predictors.
We'll also increase the number of predictors to 100,
and suppose there are blocks of 50, 30, 10, 5, and 5 variables
that are correlated with each other with $\rho=0.75$.

```{r sim_correlated_test}
n <- 100
var_blocks <- c(rep(1,5), rep(2,5), rep(3,10), rep(4,30), rep(5, 50))
rho <- 0.75
Sigma <- .001 * (rho * outer(var_blocks, var_blocks, "=="))
nx <- 100
xx <- sort(runif(nx))
f <- function (x) { sapply(1:n, function (u) { 1/(1+x)^u }) }
fnoise <- function (x) { f(x) + mvrnorm(NROW(x), mu=rep(0,n), Sigma=Sigma) }
yy <- fnoise(xx)
matplot(xx, yy, type='l', 
        xlab='x', ylab='y', lty=1)
matlines(xx, f(xx), lty=3)
```

First let's check if we can get the covariance matrix from these data.
We'll do this by subtracting off the running mean and then taking the usual covariance.
```{r get_cov}
win_halfwidth <- 5
win_width <- 2*win_halfwidth + 1
use_these <- ((1:nrow(yy)) > win_halfwidth) & ((1:nrow(yy)) <= nrow(yy)-win_halfwidth)
zz <- apply(yy, 2, cumsum)
zz <- (zz[-(1:(win_width-1)),] - rbind(0,zz)[-((nrow(zz)-win_width+2):(nrow(zz)+1)),]) / win_width
est_cov <- cov(yy[use_these,] - zz)

matplot(xx, yy, type='l', 
        xlab='x', ylab='y', lty=1)
matlines(xx[use_these], zz, lty=3)

layout(t(1:2))
image(Sigma, asp=1, main='true covariance matrix')
image(est_cov, asp=1, main='estimated covariance matrix')
hist(est_cov[Sigma>0], xlab='covariance', main='nonzero elements')
hist(est_cov[Sigma==0], xlab='covariance', main='zero elements')
```

Now let's see how well this does despite this unmodeled correlation.
```{r error_correlated_case, fig.width=fig.dim}
ntest <- 1000
xref <- runif(ntest)
yref <- fnoise(xref)
xpred <- inverse_interpolation(xx, yy, yref, omega=0.05)
plot(xref, xpred, xlab='true x', ylab='predicted x', asp=1)
abline(0,1)
```

How does this depend on the bandwidth?
```{r error_correlated_with_n}
ntest <- 1000
omega_vals <- (1:30)/100
resid_list <- lapply(omega_vals, function (omega) {
                xpred <- inverse_interpolation(xx, yy, yref, omega=omega)
                return(xpred-xref)
            } )
names(resid_list) <- omega_vals
boxplot(resid_list, xlab='number of training samples', ylab='error')
plot(omega_vals, sapply(resid_list, sd),
        xlab=expression(omega), ylab="SD(error)", type='b')
```
